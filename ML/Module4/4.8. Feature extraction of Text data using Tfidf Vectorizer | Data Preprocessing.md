![image](https://github.com/user-attachments/assets/21d3d376-f946-4203-9d46-3363e99b73f8)

Computer can understand the numerical data and it is hard to understand the text data 
how we can implement this feature extraction of text data using tf idf vectorizer okay so i will also be showing you how you
can implement this in python okay so this also contain amazon part okay so first of all let's try to understand about this feature extraction so feature extraction is all about the mapping from textual data to real valued vectors is called as feature extraction

here i have mentioned here that to count the number of times each word appears in a document okay so what we basically do is we create a list of all the words in the paragraph or in the document and we count the number of times the words repeats okay so you may have a doubt so how how does counting the number of words can help us convert this text into numerical data so i'll give you an example let's say that we are building a machine learning model that can predict whether a male is a
spam male or a normal male okay so we all would have encountered this spam mail in our daily life and we can say that the spam mails contain the words like offers free discounts and such kind of things and a normal mail so the mail sent by our family members or our you know colleagues doesn't have these kind of words right so what happens is when you count these words it can tell the machine learning algorithm like this particular kind of label as this kind of watch say for example the label for this spam in prediction will
be spam mails and normal mail site and here this families will be the label and this can tell our model that this spam mails as the words like free discount offers etc and this is how counting the words can uh help the machine learning model to understand what is present in the data set
another important thing to note here when we do this vectorizer it doesn't understand the context of the paragraph it just uh you know tries to count the number of words so number of times the words is repeated and it doesn't understand the context

![image](https://github.com/user-attachments/assets/d971916a-d4c5-4145-9426-fcf9a3372e83)

there can be words like you know the articles and nouns and other kind of things so words like this are the a etc and etc so these words would be repeated a lot of time and we don't want to give a significant you know focus to these words and this is where we use this inverse document frequency where if a word is repeated a lot of times that word will have a small value okay so this will tell us the machine learning model that that word is not significant 

About the Dataset:
1. id: unique id for a news article
2. title: the title of a news article
3. author: author of the news article
4. text: the text of the article; could be incomplete
5. label: a label that marks whether the news article is real or fake:
           1: Fake news
           0: real News
```
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer



import nltk
nltk.download('stopwords')


# printing the stopwords in English
print(stopwords.words('english'))


Data Pre-processing

# loading the dataset to a pandas DataFrame
news_dataset = pd.read_csv('/content/train.csv')

news_dataset.shape


# print the first 5 rows of the dataframe
news_dataset.head()


# counting the number of missing values in the dataset
news_dataset.isnull().sum()


# replacing the null values with empty string
news_dataset = news_dataset.fillna('')

# merging the author name and news title
news_dataset['content'] = news_dataset['author']+' '+news_dataset['title']

print(news_dataset['content'])

# separating the data & label
X = news_dataset.drop(columns='label', axis=1)
Y = news_dataset['label']

print(X)
print(Y)

Stemming:
Stemming is the process of reducing a word to its Root word
example:
actor, actress, acting --> act


port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content 
                       if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_dataset['content'] = news_dataset['content'].apply(stemming)

print(news_dataset['content'])

#separating the data and label
X = news_dataset['content'].values
Y = news_dataset['label'].values

print(X)

print(Y)

Y.shape

Tf-Idf

# convert the textual data to Feature Vectors
vectorizer = TfidfVectorizer()

vectorizer.fit(X)
X = vectorizer.transform(X)

print(X)


```
